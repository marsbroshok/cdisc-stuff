{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip install -U tensorflow\n",
    "pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1.3.0', '2.0.8')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D, Input\n",
    "from keras.optimizers import Adam, Adagrad, RMSprop, SGD\n",
    "\n",
    "print(tf.__version__, keras.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import google.datalab.storage as storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global vars\n",
    "BUCKET = 'aus-cloud-ml-handson'\n",
    "PREFIX = 'cdisc-data/bottleneck-features/'\n",
    "SUFFIX = '.npy'  # numpy saved array file extension\n",
    "GCS_PATH = 'gs://{}/{}'.format(BUCKET, PREFIX)\n",
    "DATA_PATH = GCS_PATH + 'input-ssd/'\n",
    "TRAIN_BSON_PATH = DATA_PATH + 'train.bson'\n",
    "TEST_BSON_PATH = DATA_PATH + 'test.bson'\n",
    "NUM_TRAIN_PRODUCTS = 7069896\n",
    "NUM_TEST_PRODUCTS = 1768172\n",
    "IMG_WIDTH = IMG_HEIGTH = 180\n",
    "NUM_CLASSES = 5270\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- Read bottleneck features and labels\n",
    "- List all files in alphabetical order\n",
    "- Build classifier NN model\n",
    "- Merge predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read bottleneck features and labels\n",
    "# List all files in alphabetical order\n",
    "def read_files_from_gcs(bucket, prefix, suffix=''):\n",
    "  gcs_bucket = storage.Bucket(bucket)\n",
    "  all_files = gcs_bucket.objects(prefix=prefix)\n",
    "  all_files = (obj.key for obj in all_files if suffix in obj.key)\n",
    "  return all_files\n",
    "\n",
    "def load_numpy_gcs(full_filepath):\n",
    "  with tf.gfile.Open(full_filepath) as f:\n",
    "     return np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train_features = read_files_from_gcs(bucket=BUCKET, prefix=PREFIX+'train_features_')\n",
    "# train_features = np.array(list(train_features))\n",
    "\n",
    "# train_labels = read_files_from_gcs(bucket=BUCKET, prefix=PREFIX+'train_labels_')\n",
    "# train_labels = np.array(list(train_labels))\n",
    "\n",
    "# test_features = read_files_from_gcs(bucket=BUCKET, prefix=PREFIX+'test_features_')\n",
    "# test_features = np.array(list(test_features))\n",
    "\n",
    "# train_features.shape, train_labels.shape, test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save file lists to not reload again (slow!)\n",
    "np.savetxt('./train_features.txt', train_features, fmt='%s')\n",
    "np.savetxt('./train_labels.txt', train_labels, fmt='%s')\n",
    "np.savetxt('./test_features.txt', test_features, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load file lists\n",
    "train_features = np.loadtxt('./train_features.txt', dtype='str')\n",
    "train_labels = np.loadtxt('./train_labels.txt', dtype='str')\n",
    "test_features = np.loadtxt('./test_features.txt', dtype='str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_batches_from_gcs(fn_features, fn_labels):\n",
    "  fn_features = 'gs://'+BUCKET+'/'+fn_features\n",
    "  fn_labels = 'gs://'+BUCKET+'/'+fn_labels\n",
    "  file_id = '_'.join(fn_features.split('_')[-2:])\n",
    "  if file_id not in fn_labels: \n",
    "    raise Exception(\"Files not matching! {} {}\".format(fn_features, fn_labels))  # sanity check\n",
    "\n",
    "  bx = load_numpy_gcs(fn_features)\n",
    "  by = load_numpy_gcs(fn_labels)\n",
    "  return bx, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((131, 6, 6, 2048), (131,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx, by = read_batches_from_gcs(train_features[1], train_labels[1])\n",
    "by = np.argmax(by, axis=1)\n",
    "\n",
    "bx.shape, by.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6, 2048)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batches_iterator(train_features, train_labels, random=True):\n",
    "  fn_stacked = np.stack((train_features, train_labels), axis=1)\n",
    "  if random:\n",
    "    fn_stacked = np.random.permutation(fn_stacked)\n",
    "  for fn_item in fn_stacked:\n",
    "    train_features = fn_item[0]\n",
    "    train_labels = fn_item[1]\n",
    "    bx, by = read_batches_from_gcs(train_features, train_labels)\n",
    "    by = np.argmax(by, axis=1)\n",
    "    yield bx, by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches_iter = batches_iterator(train_features, train_labels, random=True)\n",
    "total_batches = len(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128, 6, 6, 2048), (128,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bx, by = next(batches_iter)\n",
    "bx.shape, by.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# NN model (Xception-like)\n",
    "input_layer = Input(shape=bx.shape[1:])\n",
    "x = GlobalAveragePooling2D()(input_layer)\n",
    "pred_layer = Dense(NUM_CLASSES, activation='softmax', name='predictions')(x)\n",
    "\n",
    "# Build model for provided classes\n",
    "model = Model(inputs=input_layer, outputs=pred_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 6, 6, 2048)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 5270)              10798230  \n",
      "=================================================================\n",
      "Total params: 10,798,230\n",
      "Trainable params: 10,798,230\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=1E-2), loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "6/6 [==============================] - 11s - loss: 10.1219 - acc: 0.0325    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff64bb8f190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(batches_iter, steps_per_epoch=total_batches//10000, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
